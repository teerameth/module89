{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = [224, 224, 3]\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "labelNames = [\"empty\", \"piece\"]\n",
    "labels = ['_', 'o']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import math\n",
    "from transform import order_points, poly2view_angle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Converting the values into features\n",
    "def _int64_feature(value):  # _int64 is used for numeric values\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def _bytes_feature(value):  # _bytes is used for string/char values\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "tile_feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "def _parse_image_function(example_proto):\n",
    "    data = tf.io.parse_single_example(example_proto, tile_feature_description) # Parse the input tf.train.Example proto using dictionary.\n",
    "    label = data['label']   # not need one-hot encoding (binary prediction)\n",
    "    # label = tf.one_hot(indices=data['label'], depth=NUM_CLASSES, on_value=1.0, off_value=0.0)\n",
    "    image = tf.io.decode_png(data['image'])  # Auto detect image shape when decoded\n",
    "    output = {}\n",
    "    # return [image], [label]\n",
    "    return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset_config = json.load(open(os.path.join('../../config/dataset_config.json')))\n",
    "output_path = dataset_config['capture_path']\n",
    "top_tfrecord_pattern = os.path.join(output_path, 'top_random.tfrecords-????-of-????')\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy() # Not connected to a TPU runtime. Using CPU/GPU strategy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 11:56:03.230635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.231425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.239626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.240257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.240723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.241247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.242095: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-28 11:56:03.366020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.366500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.366877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.367304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.367679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:03.368096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.092788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.093292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.093668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.094084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.094438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.094837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9781 MB memory:  -> device: 0, name: NVIDIA Graphics Device, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2022-04-28 11:56:04.095092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-28 11:56:04.095431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7405 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: /media/teera/SSD250GB/dataset/module89/V5/top_random.tfrecords-????-of-????'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m shards \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFRecordDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtop_tfrecord_pattern\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m shards \u001B[38;5;241m=\u001B[39m shards\u001B[38;5;241m.\u001B[39mshuffle(buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m) \u001B[38;5;66;03m# Make sure to fully shuffle the list of tfrecord files.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Preprocesses 10 files concurrently and interleaves records from each file into a single, unified dataset.\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:1348\u001B[0m, in \u001B[0;36mDatasetV2.list_files\u001B[0;34m(file_pattern, shuffle, seed, name)\u001B[0m\n\u001B[1;32m   1341\u001B[0m condition \u001B[38;5;241m=\u001B[39m math_ops\u001B[38;5;241m.\u001B[39mgreater(array_ops\u001B[38;5;241m.\u001B[39mshape(matching_files)[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   1342\u001B[0m                              name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmatch_not_empty\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1344\u001B[0m message \u001B[38;5;241m=\u001B[39m math_ops\u001B[38;5;241m.\u001B[39madd(\n\u001B[1;32m   1345\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo files matched pattern: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1346\u001B[0m     string_ops\u001B[38;5;241m.\u001B[39mreduce_join(file_pattern, separator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1348\u001B[0m assert_not_empty \u001B[38;5;241m=\u001B[39m \u001B[43mcontrol_flow_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAssert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcondition\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43massert_not_empty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1350\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcontrol_dependencies([assert_not_empty]):\n\u001B[1;32m   1351\u001B[0m   matching_files \u001B[38;5;241m=\u001B[39m array_ops\u001B[38;5;241m.\u001B[39midentity(matching_files)\n",
      "File \u001B[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:157\u001B[0m, in \u001B[0;36mAssert\u001B[0;34m(condition, data, summarize, name)\u001B[0m\n\u001B[1;32m    155\u001B[0m     xs \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_n_to_tensor(data)\n\u001B[1;32m    156\u001B[0m     data_str \u001B[38;5;241m=\u001B[39m [_summarize_eager(x, summarize) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m xs]\n\u001B[0;32m--> 157\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mInvalidArgumentError(\n\u001B[1;32m    158\u001B[0m         node_def\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    159\u001B[0m         op\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    160\u001B[0m         message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to be true. Summarized data: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m    161\u001B[0m         (condition, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(data_str)))\n\u001B[1;32m    162\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mname_scope(name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAssert\u001B[39m\u001B[38;5;124m\"\u001B[39m, [condition, data]) \u001B[38;5;28;01mas\u001B[39;00m name:\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: /media/teera/SSD250GB/dataset/module89/V5/top_random.tfrecords-????-of-????'"
     ]
    }
   ],
   "source": [
    "shards = tf.data.TFRecordDataset.list_files(top_tfrecord_pattern)\n",
    "shards = shards.shuffle(buffer_size=1000) # Make sure to fully shuffle the list of tfrecord files.\n",
    "# Preprocesses 10 files concurrently and interleaves records from each file into a single, unified dataset.\n",
    "dataset = shards.interleave(\n",
    "    tf.data.TFRecordDataset,\n",
    "    cycle_length=10,\n",
    "    block_length=1)\n",
    "dataset = dataset.map(_parse_image_function, num_parallel_calls=4)\n",
    "split_filter = [0, 0, 0, 0, 0, 0, 0, 1, 1, 2]   # Use to filter dataset as train:val:test = 70:20:10\n",
    "train_set = dataset.enumerate() \\\n",
    "                    .filter(lambda x,y: x % 40 <= 6) \\\n",
    "                    .map(lambda x,y: y)\n",
    "val_set = dataset.enumerate() \\\n",
    "                    .filter(lambda x,y: x % 40 == 7 or x % 10 == 8) \\\n",
    "                    .map(lambda x,y: y)\n",
    "test_set = dataset.enumerate() \\\n",
    "                    .filter(lambda x,y: x % 40 == 9) \\\n",
    "                    .map(lambda x,y: y)\n",
    "train_set = train_set.shuffle(2000)\n",
    "train_set = train_set.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_set = val_set.shuffle(2000)\n",
    "val_set = val_set.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_set = test_set.shuffle(2000)\n",
    "test_set = test_set.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "\n",
    "\n",
    "img_augmentation = Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomRotation(factor=0.15),\n",
    "        tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        tf.keras.layers.RandomFlip(),\n",
    "        tf.keras.layers.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")\n",
    "# base_model = tf.keras.applications.EfficientNetB0(input_shape=IMAGE_SIZE,\n",
    "#                                                   include_top=False,\n",
    "#                                                   weights='imagenet')\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMAGE_SIZE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "# Add a classification head\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1) # binary prediction\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=IMAGE_SIZE)\n",
    "x = img_augmentation(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "base_model.trainable = True # Unlock some layers\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))   # 154 for MobileNetV2, 237 for EfficientNetB0\n",
    "\n",
    "# unlock_at = 215\n",
    "unlock_at = 145\n",
    "learning_rate = base_learning_rate\n",
    "\n",
    "for layer in base_model.layers[:unlock_at]: layer.trainable = False     # Freeze all the layers before the `unlock_at` layer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First validation (before training: should be 16.67%)\n",
    "initial_epochs = 2\n",
    "loss0, accuracy0 = model.evaluate(test_set)\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initial Training\n",
    "history = model.fit(train_set,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=val_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unlock_at = 175\n",
    "unlock_at = 135\n",
    "learning_rate = base_learning_rate/10   # Lowering learning rate to 10% to prevent overfit\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(model.trainable_variables)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fine_tune_epochs = 2\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_set,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1]+1,\n",
    "                         validation_data=val_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_set)\n",
    "print('Test accuracy :', accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "model_config = json.load(open(os.path.join('../../config/model_config.json')))\n",
    "checkpoint_path = os.path.join(model_config['base_path'], model_config['top_classifier'])\n",
    "model.save_weights(os.path.join(checkpoint_path, \"cp-{epoch:04d}.ckpt\".format(epoch=20)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(os.path.join(checkpoint_path,'model.h5'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "savedModel = tf.keras.models.load_model(os.path.join(checkpoint_path,'model.h5'))\n",
    "savedModel.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}